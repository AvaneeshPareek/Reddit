[2025-01-19T17:02:55.592+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-01-19T17:02:55.601+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-01-19T17:02:54.091393+00:00 [queued]>
[2025-01-19T17:02:55.606+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-01-19T17:02:54.091393+00:00 [queued]>
[2025-01-19T17:02:55.606+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2025-01-19T17:02:55.611+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2025-01-19 17:02:54.091393+00:00
[2025-01-19T17:02:55.617+0000] {standard_task_runner.py:72} INFO - Started process 65 to run task
[2025-01-19T17:02:55.621+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2025-01-19T17:02:54.091393+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp7mkgz8he']
[2025-01-19T17:02:55.622+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask reddit_extraction
[2025-01-19T17:02:55.683+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-01-19T17:02:54.091393+00:00 [running]> on host 528e17a0e135
[2025-01-19T17:02:55.727+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Avaneesh Pareek' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2025-01-19T17:02:54.091393+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-01-19T17:02:54.091393+00:00'
[2025-01-19T17:02:55.728+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-01-19T17:02:55.737+0000] {logging_mixin.py:190} INFO - Connected to Reddit
[2025-01-19T17:02:57.064+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cqao8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Life of a Data Engineer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4asfr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'ups': 720, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 720, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/E_j8AK68T9Kk_7TSrGeQiaaFoV-si84vW-molakH9-I.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1737216596.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/26hsel7u0sde1.gif', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?format=png8&s=5a3986745d9aa12b291149f2467899625481eeec', 'width': 900, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=108&crop=smart&format=png8&s=026bb017686bdf05330ca1483907b026262294d1', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=216&crop=smart&format=png8&s=ef03d229100fe569d5ed873cb1c340452160b326', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=320&crop=smart&format=png8&s=4d10ef7519214abb60fd89a9c46535fd16b825f1', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=640&crop=smart&format=png8&s=8876a40fe55ee64749c403b760f7128783bde3f5', 'width': 640, 'height': 640}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?s=c2829574c128a9d99af901962b7e17966359ef69', 'width': 900, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=108&crop=smart&s=02233dfff9206bcfca9166cfec8f6135d10bf462', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=216&crop=smart&s=0b420060cff270df66c8d0169037d86ed1fea35e', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=320&crop=smart&s=cf95a465856729737f61f717b4a01a86441f7ddf', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=640&crop=smart&s=c61cee5bed6b5fc75a002d0f51ce029e0602da04', 'width': 640, 'height': 640}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?format=mp4&s=873c58fa8ca7333186f82cbcebd45e55b09e38dc', 'width': 900, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=108&format=mp4&s=d7251504cb238f534597d2975cca2ca201e65b8e', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=216&format=mp4&s=a535d1952d93693141af73a88ef5c681a88d9072', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=320&format=mp4&s=3002f3b5c4a187a22f02833530a86042ebefcf12', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/26hsel7u0sde1.gif?width=640&format=mp4&s=c9050c2ba4f8958dfa1e89fd2acf2ce967e524da', 'width': 640, 'height': 640}]}}, 'id': 'glt6BC0ogKzDlJTlKjVdIQJCLwTdyhbyWFkDReumVoc'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1i4asfr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dan_the_lion'), 'discussion_type': None, 'num_comments': 31, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4asfr/life_of_a_data_engineer/', 'stickied': False, 'url': 'https://i.redd.it/26hsel7u0sde1.gif', 'subreddit_subscribers': 246994, 'created_utc': 1737216596.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.065+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Throughout my career, when I come across data pipelines that are purely python, I see slightly more of them use OOP/Classes than I do see Functional Programming style. \n\nBut the class based ones only seem to instantiate the class one time. I’m not a design pattern expert but I believe this is called a singleton? \n\nSo what I’m trying to understand is, “when” should a data pipeline be OOP Vs. Functional Programming style? \n\nIf you’re only instantiating a class once, shouldn’t you just use functional programming instead of OOP? \n\nI’m seeing less and less data pipelines in pure python (exception being PySpark data pipelines) but when I do see them, this is something I’ve noticed. ', 'author_fullname': 't2_8wpw0e1t', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Are most Data Pipelines in python OOP or Functional? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4njkr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 92, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 92, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1737252517.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737252276.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Throughout my career, when I come across data pipelines that are purely python, I see slightly more of them use OOP/Classes than I do see Functional Programming style. </p>\n\n<p>But the class based ones only seem to instantiate the class one time. I’m not a design pattern expert but I believe this is called a singleton? </p>\n\n<p>So what I’m trying to understand is, “when” should a data pipeline be OOP Vs. Functional Programming style? </p>\n\n<p>If you’re only instantiating a class once, shouldn’t you just use functional programming instead of OOP? </p>\n\n<p>I’m seeing less and less data pipelines in pure python (exception being PySpark data pipelines) but when I do see them, this is something I’ve noticed. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4njkr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='khaili109'), 'discussion_type': None, 'num_comments': 47, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4njkr/are_most_data_pipelines_in_python_oop_or/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4njkr/are_most_data_pipelines_in_python_oop_or/', 'subreddit_subscribers': 246994, 'created_utc': 1737252276.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.066+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm trying to understand how DSA concepts are actually used on the job.", 'author_fullname': 't2_20po460h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "Why are DSA questions being asked in interviews if it's not so extensively used on the job?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4m272', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 32, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 32, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737247717.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m trying to understand how DSA concepts are actually used on the job.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4m272', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='TheThinker12'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4m272/why_are_dsa_questions_being_asked_in_interviews/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4m272/why_are_dsa_questions_being_asked_in_interviews/', 'subreddit_subscribers': 246994, 'created_utc': 1737247717.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.067+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, I've read several articles about those topics, however I would like to ask Fabric practitioners what is the best approach to these 3 issues. I need to create medallion architecture where I create seperate Lakehouse for bronze and silver layer and Data Warehouse (or Lakehouse) for gold layer. Here are my questions:\n\n**1st - creating separate workspaces for bronze/silver/gold layer in Fabric**\n\nIt's recommended to create separate Lakehouses in separate workspaces for each medallion layer - bronze, silver and gold. I'm wondering how it corresponds to another quite common pattern to create separate workspaces for Development, Test and Production (deployment pipeline). How should I combine the two approaches? In my company we split workspaces into DEV/TEST/PROD. I thought about 2 approaches:\n\n*1. create 3 workspaces for bronze/silver/gold layers and within each create Lakehouses for DEV, TEST and PROD.* Here we follow the recommendation of having 3 separate workspaces for each medallion layer. For example: \n\nBRONZE workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD (in separate folders for example)\n\nSILVER workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD\n\nGOLD workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD\n\n*2. create 9 workspaces for each medallion layer combined with dev/test/prod architecture.* For example:\n\nfirst workspace: Lakehouse BRONZE Dev\n\nsecond workspace: Lakehouse BRONZE Test\n\nanother workspace: Lakehouse BRONZE Prod\n\nanother workspace: Lakehouse SILVER Dev\n\nanother workspace: Lakehouse SILVER Test\n\netc...\n\nHere we also follow recommendation of having separate workspaces for each layer. However, as a result we have 9 workspaces. I'm wondering how those 2 approaches works in case we would use deployment pipeline to manage DEV/TEST/PROD environments. Please advise which approach is best here.\n\n**2nd - data ingestion to bronze layer**\n\nLet's say I created Lakehouse in bronze layer. Now I would like to load data efficiently to this Lakehouse. When it comes to data source it would be SAP data (to be precise data coming from SAP BW Application Server, de facto OLAP Cubes). I can connect to SAP via Dataflow connector. The issue is that I don't want to use Dataflows which are slow are generate overhead (I load huge amount of data). So please advise me how to efficiently load those data directly to Lakehouse Bronze layer from SAP. I have 2 options on my mind:\n\n1. using data pipeline and Copy data activity to ingest data. However, SAP BW Application Server isn't available for data pipeline so I guess this option is about to be dropped\n\n2. using PySpark and Notebooks - I could directly retrieve data from SAP BW Application Server and load it to Lakehouse as .parquet files. Question is if I could make connection to this particular SAP Server from Notebook (PySpark) or not? As far as I know Spark works much faster that Dataflows and is better cost-wise, that's why I think about this option.\n\n**3rd - incremental data load to silver layer**\n\nNow I need to load data from bronze to silver layer. Initial load to bronze layer would embrace, let's say, data for 2 years. Then I would like to upload data to silver layer incrementally for last 3 months. So now as a first step I should load data for 2 last years to bronze layer and then load it to silver layer. Next, delete all 2 years data from bronze layer. In next step load latest data for 3 months to bronze layer and then refresh last 3 months in silver layer. So in bronze layer we would always have data for latest 3 months and in silver layer data for last 2 years (from now) where last 3 months are updated and up-to-date.\n\nMy question is if it's good approach to incremental refresh and MOST importantly - should I make it in PySpark or use another approach?", 'author_fullname': 't2_huioqzbu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Real-world patterns for creating medallion workspaces and ingest data in Fabric', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4klij', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737243368.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, I&#39;ve read several articles about those topics, however I would like to ask Fabric practitioners what is the best approach to these 3 issues. I need to create medallion architecture where I create seperate Lakehouse for bronze and silver layer and Data Warehouse (or Lakehouse) for gold layer. Here are my questions:</p>\n\n<p><strong>1st - creating separate workspaces for bronze/silver/gold layer in Fabric</strong></p>\n\n<p>It&#39;s recommended to create separate Lakehouses in separate workspaces for each medallion layer - bronze, silver and gold. I&#39;m wondering how it corresponds to another quite common pattern to create separate workspaces for Development, Test and Production (deployment pipeline). How should I combine the two approaches? In my company we split workspaces into DEV/TEST/PROD. I thought about 2 approaches:</p>\n\n<p><em>1. create 3 workspaces for bronze/silver/gold layers and within each create Lakehouses for DEV, TEST and PROD.</em> Here we follow the recommendation of having 3 separate workspaces for each medallion layer. For example: </p>\n\n<p>BRONZE workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD (in separate folders for example)</p>\n\n<p>SILVER workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD</p>\n\n<p>GOLD workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD</p>\n\n<p><em>2. create 9 workspaces for each medallion layer combined with dev/test/prod architecture.</em> For example:</p>\n\n<p>first workspace: Lakehouse BRONZE Dev</p>\n\n<p>second workspace: Lakehouse BRONZE Test</p>\n\n<p>another workspace: Lakehouse BRONZE Prod</p>\n\n<p>another workspace: Lakehouse SILVER Dev</p>\n\n<p>another workspace: Lakehouse SILVER Test</p>\n\n<p>etc...</p>\n\n<p>Here we also follow recommendation of having separate workspaces for each layer. However, as a result we have 9 workspaces. I&#39;m wondering how those 2 approaches works in case we would use deployment pipeline to manage DEV/TEST/PROD environments. Please advise which approach is best here.</p>\n\n<p><strong>2nd - data ingestion to bronze layer</strong></p>\n\n<p>Let&#39;s say I created Lakehouse in bronze layer. Now I would like to load data efficiently to this Lakehouse. When it comes to data source it would be SAP data (to be precise data coming from SAP BW Application Server, de facto OLAP Cubes). I can connect to SAP via Dataflow connector. The issue is that I don&#39;t want to use Dataflows which are slow are generate overhead (I load huge amount of data). So please advise me how to efficiently load those data directly to Lakehouse Bronze layer from SAP. I have 2 options on my mind:</p>\n\n<ol>\n<li><p>using data pipeline and Copy data activity to ingest data. However, SAP BW Application Server isn&#39;t available for data pipeline so I guess this option is about to be dropped</p></li>\n<li><p>using PySpark and Notebooks - I could directly retrieve data from SAP BW Application Server and load it to Lakehouse as .parquet files. Question is if I could make connection to this particular SAP Server from Notebook (PySpark) or not? As far as I know Spark works much faster that Dataflows and is better cost-wise, that&#39;s why I think about this option.</p></li>\n</ol>\n\n<p><strong>3rd - incremental data load to silver layer</strong></p>\n\n<p>Now I need to load data from bronze to silver layer. Initial load to bronze layer would embrace, let&#39;s say, data for 2 years. Then I would like to upload data to silver layer incrementally for last 3 months. So now as a first step I should load data for 2 last years to bronze layer and then load it to silver layer. Next, delete all 2 years data from bronze layer. In next step load latest data for 3 months to bronze layer and then refresh last 3 months in silver layer. So in bronze layer we would always have data for latest 3 months and in silver layer data for last 2 years (from now) where last 3 months are updated and up-to-date.</p>\n\n<p>My question is if it&#39;s good approach to incremental refresh and MOST importantly - should I make it in PySpark or use another approach?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4klij', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='muskagap2'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4klij/realworld_patterns_for_creating_medallion/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4klij/realworld_patterns_for_creating_medallion/', 'subreddit_subscribers': 246994, 'created_utc': 1737243368.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.068+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Recently I have seen that in the second or third round companies are asking open ended one liners, how should we approach these? Apart from the Data model, they are also expecting that we make some low level code and APIs and then also explain them about why we chose one service over the other, explain about latency, throughput stuff etc.  \nExamples:\n\n1. Design a Rider Management Platform which will provide riders to different e-commerce/quick-commerce websites\n\n2. Design DQ Framework.\n\n3. Design whatsapp/instagram/twitter/Jira etc.\n\nAs someone who wants to grow more into this field, I mean like as I grow I want to be on the tech side only like Data Engineer - > Senior Data Engineer - > Lead/Staff DE -> Principal DE. How should we proceed? Because these questions are similar to the System design rounds for Software profiles. Also, I feel that next 5 years DE profile is going to be merged in to Software roles only, like we see in some companies with designation such as Data Software engineer or Software Engineer (Data).\n\nSeeking guidance from Senior folks here.  \n', 'author_fullname': 't2_10a7kt0tvu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to handle open ended design questions for Data roles and way forward?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4xbdk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737289436.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Recently I have seen that in the second or third round companies are asking open ended one liners, how should we approach these? Apart from the Data model, they are also expecting that we make some low level code and APIs and then also explain them about why we chose one service over the other, explain about latency, throughput stuff etc.<br/>\nExamples:</p>\n\n<ol>\n<li><p>Design a Rider Management Platform which will provide riders to different e-commerce/quick-commerce websites</p></li>\n<li><p>Design DQ Framework.</p></li>\n<li><p>Design whatsapp/instagram/twitter/Jira etc.</p></li>\n</ol>\n\n<p>As someone who wants to grow more into this field, I mean like as I grow I want to be on the tech side only like Data Engineer - &gt; Senior Data Engineer - &gt; Lead/Staff DE -&gt; Principal DE. How should we proceed? Because these questions are similar to the System design rounds for Software profiles. Also, I feel that next 5 years DE profile is going to be merged in to Software roles only, like we see in some companies with designation such as Data Software engineer or Software Engineer (Data).</p>\n\n<p>Seeking guidance from Senior folks here.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4xbdk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='jaina15'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4xbdk/how_to_handle_open_ended_design_questions_for/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4xbdk/how_to_handle_open_ended_design_questions_for/', 'subreddit_subscribers': 246994, 'created_utc': 1737289436.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.068+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_5fn6v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'We added parquet support (shoutout to the great hyparquet package) to our data management/app building tool, interested in feedback on workflow and anything else. Here working on the flights-1m dataset.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4vij8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'reddit_video': {'bitrate_kbps': 5000, 'fallback_url': 'https://v.redd.it/km942he6fxde1/DASH_1080.mp4?source=fallback', 'has_audio': False, 'height': 952, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/km942he6fxde1/DASH_96.mp4', 'dash_url': 'https://v.redd.it/km942he6fxde1/DASHPlaylist.mpd?a=1739898176%2CZTU5YmYzODY0NjY5N2FhMzYwMmZmZTFhODY0YmYxOWQ4OGRlZTZhNzM4ZmUxODVmYWY5OWQ2NjI4N2ZiMmVhNA%3D%3D&v=1&f=sd', 'duration': 86, 'hls_url': 'https://v.redd.it/km942he6fxde1/HLSPlaylist.m3u8?a=1739898176%2CZjExNzY2YjY3Njg3M2RjMmNlNTVhMmU3ZmNmMzIxOGI3YjJiZjc5ZGE3ZTU4OTgwMDRlMzg3NzkwNWZmYzQxYg%3D%3D&v=1&f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=140&height=69&crop=140:69,smart&format=jpg&v=enabled&lthumb=true&s=5273b11b722e35f570f34812efe431a9122ff626', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'hosted:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1737281986.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'v.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://v.redd.it/km942he6fxde1', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?format=pjpg&auto=webp&s=62b8b04fc8d758a253b40dd7b942b3df8fe78cb5', 'width': 2560, 'height': 1270}, 'resolutions': [{'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=108&crop=smart&format=pjpg&auto=webp&s=68641393df86af94047ed8770ed41193380bc8a8', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=216&crop=smart&format=pjpg&auto=webp&s=39f51f20e7743df8a2c82709223af3d2692b863b', 'width': 216, 'height': 107}, {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=320&crop=smart&format=pjpg&auto=webp&s=bdca2c495fdd2861cf188578da2566955e4871e4', 'width': 320, 'height': 158}, {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=640&crop=smart&format=pjpg&auto=webp&s=6f72e447d8f728ac7d63b4a941ec9b9e611b2eea', 'width': 640, 'height': 317}, {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=960&crop=smart&format=pjpg&auto=webp&s=ab343ad4bbcfc2f815551bd1650f35a843a4a977', 'width': 960, 'height': 476}, {'url': 'https://external-preview.redd.it/dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0.png?width=1080&crop=smart&format=pjpg&auto=webp&s=1c5efccafe119f0e24a8c3d6beb8badf1ac940d2', 'width': 1080, 'height': 535}], 'variants': {}, 'id': 'dmZ1eXRnZTZmeGRlMT7kCHVWswfQZCBcB2t6mOgGuH6fra1OI8zLX3TDcia0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1i4vij8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='fudgem'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4vij8/we_added_parquet_support_shoutout_to_the_great/', 'stickied': False, 'url': 'https://v.redd.it/km942he6fxde1', 'subreddit_subscribers': 246994, 'created_utc': 1737281986.0, 'num_crossposts': 0, 'media': {'reddit_video': {'bitrate_kbps': 5000, 'fallback_url': 'https://v.redd.it/km942he6fxde1/DASH_1080.mp4?source=fallback', 'has_audio': False, 'height': 952, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/km942he6fxde1/DASH_96.mp4', 'dash_url': 'https://v.redd.it/km942he6fxde1/DASHPlaylist.mpd?a=1739898176%2CZTU5YmYzODY0NjY5N2FhMzYwMmZmZTFhODY0YmYxOWQ4OGRlZTZhNzM4ZmUxODVmYWY5OWQ2NjI4N2ZiMmVhNA%3D%3D&v=1&f=sd', 'duration': 86, 'hls_url': 'https://v.redd.it/km942he6fxde1/HLSPlaylist.m3u8?a=1739898176%2CZjExNzY2YjY3Njg3M2RjMmNlNTVhMmU3ZmNmMzIxOGI3YjJiZjc5ZGE3ZTU4OTgwMDRlMzg3NzkwNWZmYzQxYg%3D%3D&v=1&f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_video': True, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.069+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Qlik acquired Talend and has integrated it into their data studio for ingestion (currently limited to databases afaik).\n\nSince our organization already uses Qlik, this was introduced to us last year. One of our team members spent a few weeks exploring it and concluded that it had its rough edges and was difficult to troubleshoot if something went wrong.\n\nNow, it's being showed down our throats as it has been decided that Talend will be used for ingestions, because management believes UI-based tools equate to faster results. To counter this, I built a few ingestion pipelines for sources that Talend cannot handle (such as Rest APIs) using [dlt](https://github.com/dlt-hub/dlt) that have been working just fine since ~October of last year, but it seems that has fallen on deaf ears.\n\nHas anyone here used the Qlik-integrated version of Talend? What’s your experience with it?", 'author_fullname': 't2_5zhaa3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you think of Talend?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4url5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737278654.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Qlik acquired Talend and has integrated it into their data studio for ingestion (currently limited to databases afaik).</p>\n\n<p>Since our organization already uses Qlik, this was introduced to us last year. One of our team members spent a few weeks exploring it and concluded that it had its rough edges and was difficult to troubleshoot if something went wrong.</p>\n\n<p>Now, it&#39;s being showed down our throats as it has been decided that Talend will be used for ingestions, because management believes UI-based tools equate to faster results. To counter this, I built a few ingestion pipelines for sources that Talend cannot handle (such as Rest APIs) using <a href="https://github.com/dlt-hub/dlt">dlt</a> that have been working just fine since ~October of last year, but it seems that has fallen on deaf ears.</p>\n\n<p>Has anyone here used the Qlik-integrated version of Talend? What’s your experience with it?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?auto=webp&s=d5accaf031f0474a61cc633c4916a1f3f05b09ba', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=108&crop=smart&auto=webp&s=50d43985b52a793d406021375f2fcfff0e61935e', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=216&crop=smart&auto=webp&s=003164c5b6bb74d7e13c1d0ed3d6e7d59574b129', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=320&crop=smart&auto=webp&s=366495c0b4daf5629544e70d72170da63eaaa97e', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=640&crop=smart&auto=webp&s=08cb6fc5c4f7360c3caa1da88e0f00cfc0c2f772', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=960&crop=smart&auto=webp&s=e7a670a1833047b39699159068ee3fa77a1ce511', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/vj2R9y9c15ZWwQ0rVNbjnqxonzhVqtB_u0xWTUZQ7pg.jpg?width=1080&crop=smart&auto=webp&s=46d8010b4974b96ba8faa4ab5384f6eb35951857', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'dGe3tDYoD14uQuuhjFYVHktTLFrbAr1dhd6HH6bWRkQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4url5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='laegoiste'), 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4url5/what_do_you_think_of_talend/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4url5/what_do_you_think_of_talend/', 'subreddit_subscribers': 246994, 'created_utc': 1737278654.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.069+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I’ve got an annual learning credit through work (about $500) that I can use for any courses, certifications, etc. \n\nWould love recommendations for how to use this. I’m in more of an analytics engineering role currently, but want to expand more into the true data eng/platform/infra side. Some of the topics I’d be interested in learning more about include: \n\n- Kafka -> Spark Streaming -> DataBricks Delta Lake\n- AWS for Data Engineering \n- Data Platform/Infra - I’d really love to learn the end-to-end required with deploying an Airflow instance onto a Kubernetes cluster. Would love anything that combines Airflow config with Terraform, EKS, etc. \n- Dev Ops - CI/CD, IaC\n\nEDIT: adding some more info on my background. I’ve been in a few DE/analytics engineering positions in big tech for the past 5ish years, so looking for some more advanced courses beyond the basics/intros. I’ve toyed around with most things in my list above on my own - I know the best way to learn is through projects, but I’m specifically looking for how to spend this learning credit ', 'author_fullname': 't2_4vl67vbp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Annual learning credit', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4qg8h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1737265091.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737261447.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I’ve got an annual learning credit through work (about $500) that I can use for any courses, certifications, etc. </p>\n\n<p>Would love recommendations for how to use this. I’m in more of an analytics engineering role currently, but want to expand more into the true data eng/platform/infra side. Some of the topics I’d be interested in learning more about include: </p>\n\n<ul>\n<li>Kafka -&gt; Spark Streaming -&gt; DataBricks Delta Lake</li>\n<li>AWS for Data Engineering </li>\n<li>Data Platform/Infra - I’d really love to learn the end-to-end required with deploying an Airflow instance onto a Kubernetes cluster. Would love anything that combines Airflow config with Terraform, EKS, etc. </li>\n<li>Dev Ops - CI/CD, IaC</li>\n</ul>\n\n<p>EDIT: adding some more info on my background. I’ve been in a few DE/analytics engineering positions in big tech for the past 5ish years, so looking for some more advanced courses beyond the basics/intros. I’ve toyed around with most things in my list above on my own - I know the best way to learn is through projects, but I’m specifically looking for how to spend this learning credit </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1i4qg8h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DataAnalCyst'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4qg8h/annual_learning_credit/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4qg8h/annual_learning_credit/', 'subreddit_subscribers': 246994, 'created_utc': 1737261447.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.070+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi! Noob looking for advice. My goal is to train myself to be able to build a data platform and analytics platform from scratch. I'm not looking to build something too complicated - think of use cases for small businesses like a saas startup. \n\nI'd like to hear your suggestions on good learning materials. I'm looking for courses that can help me understand:\n\n* What a data infra typically looks like, what's different components, how they are connected, mainstream tools, etc.\n* How to make decisions on what tools / framework to use for each component, what's the trade-offs to be considered, best practices for scaling, etc. (i.e. I don't want to just build something that works, but want to understand the implications behind it)\n\nI work in analytics so am very comfortable with python and sql, but have almost no knowledge on data engineering. And sadly this isn't something I can learn on the job as the data infra in my company seems way too advanced for my use case. Any pointers is appreciated. Thank you in advance!", 'author_fullname': 't2_1r13qadb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ' Beginner seeking advice: best learning resources to build data platform from scratch?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4k0z0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737241692.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi! Noob looking for advice. My goal is to train myself to be able to build a data platform and analytics platform from scratch. I&#39;m not looking to build something too complicated - think of use cases for small businesses like a saas startup. </p>\n\n<p>I&#39;d like to hear your suggestions on good learning materials. I&#39;m looking for courses that can help me understand:</p>\n\n<ul>\n<li>What a data infra typically looks like, what&#39;s different components, how they are connected, mainstream tools, etc.</li>\n<li>How to make decisions on what tools / framework to use for each component, what&#39;s the trade-offs to be considered, best practices for scaling, etc. (i.e. I don&#39;t want to just build something that works, but want to understand the implications behind it)</li>\n</ul>\n\n<p>I work in analytics so am very comfortable with python and sql, but have almost no knowledge on data engineering. And sadly this isn&#39;t something I can learn on the job as the data infra in my company seems way too advanced for my use case. Any pointers is appreciated. Thank you in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4k0z0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='datamonkeys'), 'discussion_type': None, 'num_comments': 5, 'send_replies': False, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4k0z0/beginner_seeking_advice_best_learning_resources/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4k0z0/beginner_seeking_advice_best_learning_resources/', 'subreddit_subscribers': 246994, 'created_utc': 1737241692.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.070+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello guys!  \n  \nI would like to ask for you help in order to decide whether the current data pipeline usage is good or bad.  \n  \nIn my company we use Azure stack. We gather the data from source systems (mostly Postgres and MariaDB) using Copy activity to acquire and store source data into Azure Data Lake Gen2 in parquet format.   \nAfter that we use Data Flow activity to read the data from Azure Data Lake, do some transformations and then load the data into the Data Warehouse (Synapse Analytics Dedicated SQL Pool).   \nWhat I experienced is that, by using Data Flow activity it takes some minutes (2-3 minutes) to accomplish a very basic data transformation task (such as loading fact data without having to calculate anything). Spark cluster has already been started, and TTL option is set, so it is not about instantiating a Spark cluster. The parquet files' size on the other hand are little (most of the time it does not reach 1 MB). The interesting fact is that when I monitor the Data Flow activity, the processing time is just about some seconds, but when I monitor the whole data pipeline it shows that the Data Flow activity ran for minutes, which is not effective and I think it should be run much faster.  \n  \nI tried out a new approach, which would be loading the raw data into Synapse Analytics (As Is), and then use SQL script activity to do the same transformation logic but this time the source data would be a staging table in Synapse to be able to execute SQL.  \nIt was much faster in terms of Data Pipeline execution time and also it became much cheaper approach, because i did not have to use Data Flow activity and Spark cluster.\n\nI'm curious about your opinion about the data pipeline setup. Is there any better approach than using Data Flow activity to do this data transformation? Is it better to use Azure Databricks or Synapse notebook in pyspark for this job? Or what else could you recommend? I would like to learn from your experience. Any suggestion or opinion would be much appreciated!\n\n", 'author_fullname': 't2_w1unuj9cx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure Data Factory Data Flow vs SQL script in Synapse ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4yvgd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737294701.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello guys!  </p>\n\n<p>I would like to ask for you help in order to decide whether the current data pipeline usage is good or bad.  </p>\n\n<p>In my company we use Azure stack. We gather the data from source systems (mostly Postgres and MariaDB) using Copy activity to acquire and store source data into Azure Data Lake Gen2 in parquet format.<br/>\nAfter that we use Data Flow activity to read the data from Azure Data Lake, do some transformations and then load the data into the Data Warehouse (Synapse Analytics Dedicated SQL Pool).<br/>\nWhat I experienced is that, by using Data Flow activity it takes some minutes (2-3 minutes) to accomplish a very basic data transformation task (such as loading fact data without having to calculate anything). Spark cluster has already been started, and TTL option is set, so it is not about instantiating a Spark cluster. The parquet files&#39; size on the other hand are little (most of the time it does not reach 1 MB). The interesting fact is that when I monitor the Data Flow activity, the processing time is just about some seconds, but when I monitor the whole data pipeline it shows that the Data Flow activity ran for minutes, which is not effective and I think it should be run much faster.  </p>\n\n<p>I tried out a new approach, which would be loading the raw data into Synapse Analytics (As Is), and then use SQL script activity to do the same transformation logic but this time the source data would be a staging table in Synapse to be able to execute SQL.<br/>\nIt was much faster in terms of Data Pipeline execution time and also it became much cheaper approach, because i did not have to use Data Flow activity and Spark cluster.</p>\n\n<p>I&#39;m curious about your opinion about the data pipeline setup. Is there any better approach than using Data Flow activity to do this data transformation? Is it better to use Azure Databricks or Synapse notebook in pyspark for this job? Or what else could you recommend? I would like to learn from your experience. Any suggestion or opinion would be much appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4yvgd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ancient-Low85'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4yvgd/azure_data_factory_data_flow_vs_sql_script_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4yvgd/azure_data_factory_data_flow_vs_sql_script_in/', 'subreddit_subscribers': 246994, 'created_utc': 1737294701.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.070+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've always wanted to use Airflow to manage pipelines.\n\nI want to manage several scripts in a dependency flow, but I can't find any answers on how to do it.\n\nI thought it would be a continuous series of script dependencies, like a flowchart, but I can only find answers that it can only be done through Tasks.\n\nIf I put my scripts in the task it will be huge and impossible to maintain.", 'author_fullname': 't2_ahryr1kv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[AIRFLOW] How to run one script after another?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4d4nl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737222840.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve always wanted to use Airflow to manage pipelines.</p>\n\n<p>I want to manage several scripts in a dependency flow, but I can&#39;t find any answers on how to do it.</p>\n\n<p>I thought it would be a continuous series of script dependencies, like a flowchart, but I can only find answers that it can only be done through Tasks.</p>\n\n<p>If I put my scripts in the task it will be huge and impossible to maintain.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4d4nl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Koninhooz'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4d4nl/airflow_how_to_run_one_script_after_another/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4d4nl/airflow_how_to_run_one_script_after_another/', 'subreddit_subscribers': 246994, 'created_utc': 1737222840.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.071+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey folks, I’m running into a problem when trying to read a GCP BigQuery table with AWS Glue 5.0. The table has a **BIGNUMERIC** column, and Glue fails with the error:\n\n\n\n    Data preview failure. IllegalArgumentException - BigNumeric precision is too wide (76), Spark can only handle decimal types with max precision of 38\n\n\n\nSome details about what I’ve tried:\n\n* I added a custom BigQuery connection property, {"bignumericMode":"STRING"}, hoping to force that column to be read as a string. It didn’t help—Glue still attempts to read the column as a decimal and errors out.\n* I wrote a custom query (SQL) against the same table using a SAFE\\_CAST(...) to either STRING or a narrower DECIMAL. No luck—it still throws the same precision error. \n\n\n\nChanging the source table schema isn’t possible because it’s a production table owned by another team. Has anyone run into this and found a workaround with AWS Glue 5.0 + BigQuery?\n\n\n\n**Possible ideas floating in my head:**\n\nUsing an external view in BigQuery that casts this wide BIGNUMERIC down to something Spark can handle (like a 38 precision DECIMAL or a STRING). Then Glue reads from the view. \n\nSome specialized configuration with the Spark connector that I might have overlooked. A hidden AWS Glue property or script transformation approach? If anyone has a pattern or config tweak that works, I’d be super grateful for suggestions! \n\nI’d prefer not to implement major architectural changes or replicate the table just because of a single column’s wide precision.\n\nThanks in advance.', 'author_fullname': 't2_cb6ndcnv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'AWS Glue 5.0 → BigQuery BIGNUMERIC column exceeds Spark max precision (38)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4cqtg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737221828.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks, I’m running into a problem when trying to read a GCP BigQuery table with AWS Glue 5.0. The table has a <strong>BIGNUMERIC</strong> column, and Glue fails with the error:</p>\n\n<pre><code>Data preview failure. IllegalArgumentException - BigNumeric precision is too wide (76), Spark can only handle decimal types with max precision of 38\n</code></pre>\n\n<p>Some details about what I’ve tried:</p>\n\n<ul>\n<li>I added a custom BigQuery connection property, {&quot;bignumericMode&quot;:&quot;STRING&quot;}, hoping to force that column to be read as a string. It didn’t help—Glue still attempts to read the column as a decimal and errors out.</li>\n<li>I wrote a custom query (SQL) against the same table using a SAFE_CAST(...) to either STRING or a narrower DECIMAL. No luck—it still throws the same precision error. </li>\n</ul>\n\n<p>Changing the source table schema isn’t possible because it’s a production table owned by another team. Has anyone run into this and found a workaround with AWS Glue 5.0 + BigQuery?</p>\n\n<p><strong>Possible ideas floating in my head:</strong></p>\n\n<p>Using an external view in BigQuery that casts this wide BIGNUMERIC down to something Spark can handle (like a 38 precision DECIMAL or a STRING). Then Glue reads from the view. </p>\n\n<p>Some specialized configuration with the Spark connector that I might have overlooked. A hidden AWS Glue property or script transformation approach? If anyone has a pattern or config tweak that works, I’d be super grateful for suggestions! </p>\n\n<p>I’d prefer not to implement major architectural changes or replicate the table just because of a single column’s wide precision.</p>\n\n<p>Thanks in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4cqtg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='s4lt3d_h4sh'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4cqtg/aws_glue_50_bigquery_bignumeric_column_exceeds/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4cqtg/aws_glue_50_bigquery_bignumeric_column_exceeds/', 'subreddit_subscribers': 246994, 'created_utc': 1737221828.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.072+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "At the beginning of my journey, I aspired to become a data scientist and started self-learning the field. I made some progress by learning statistics and basic machine learning concepts, including supervised and unsupervised techniques. However, I don't feel like I achieved significant expertise.\n\nLater, I joined a data analyst internship, followed by a data engineering internship. After completing these experiences, I was encouraged to pursue a data engineering career. While I’ve worked on improving my skills, I recognize that I still have a lot to learn in terms of tools and concepts to truly excel as a data engineer.\n\nRecently, I was offered a data analyst position, and I’m about to start. I feel that gaining experience in data analysis could help me build on this career path, as data science seems like a natural progression from analysis. However, I’m also drawn to data engineering. Are all these roles interconnected in the end, or should I focus on one specific path to grow my career more effectively?", 'author_fullname': 't2_krat7nvr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm Torn Between Data Career Paths\n\n", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4bkuv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.73, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737218757.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>At the beginning of my journey, I aspired to become a data scientist and started self-learning the field. I made some progress by learning statistics and basic machine learning concepts, including supervised and unsupervised techniques. However, I don&#39;t feel like I achieved significant expertise.</p>\n\n<p>Later, I joined a data analyst internship, followed by a data engineering internship. After completing these experiences, I was encouraged to pursue a data engineering career. While I’ve worked on improving my skills, I recognize that I still have a lot to learn in terms of tools and concepts to truly excel as a data engineer.</p>\n\n<p>Recently, I was offered a data analyst position, and I’m about to start. I feel that gaining experience in data analysis could help me build on this career path, as data science seems like a natural progression from analysis. However, I’m also drawn to data engineering. Are all these roles interconnected in the end, or should I focus on one specific path to grow my career more effectively?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4bkuv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Homo_Sapien98'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4bkuv/im_torn_between_data_career_paths/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4bkuv/im_torn_between_data_career_paths/', 'subreddit_subscribers': 246994, 'created_utc': 1737218757.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.073+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Sharing my 7th tech stack series article.\n\nPinterest is a great tech savy company with dozens of tech used across teams. I thought this would be great for the readers.\n\nContent is based on multiple sources including Tech Blog, Open Source websites, news articles. You will find references as you read.\n\nCouple of points:\n- The tech discussed is from multiple teams.\n- Certain aspects are not covered due to not enough information available publicly. E.g. how each system work with each other.\n- Pinterest leverages multiple tech for exabyte scala data lake.\n- Recently migrated from Druid to StarRocks.\n- StarRocks and Snowflake primary purpose is storage in this case, hence mentioned under storage.\n- Pinterest maintains their own flavor of Flink and Airflow.\n- Headsup! The article contains a sponsor.\n\n\nLet me know what I missed.\n\n\nThanks for reading.', 'author_fullname': 't2_dhgy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pinterest Data Tech Stack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1i50mw6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/EngrkMoLwFgesyfS9iJKoBuECU7w4nLhI6TomEKRka8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1737299676.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'junaideffendi.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Sharing my 7th tech stack series article.</p>\n\n<p>Pinterest is a great tech savy company with dozens of tech used across teams. I thought this would be great for the readers.</p>\n\n<p>Content is based on multiple sources including Tech Blog, Open Source websites, news articles. You will find references as you read.</p>\n\n<p>Couple of points:\n- The tech discussed is from multiple teams.\n- Certain aspects are not covered due to not enough information available publicly. E.g. how each system work with each other.\n- Pinterest leverages multiple tech for exabyte scala data lake.\n- Recently migrated from Druid to StarRocks.\n- StarRocks and Snowflake primary purpose is storage in this case, hence mentioned under storage.\n- Pinterest maintains their own flavor of Flink and Airflow.\n- Headsup! The article contains a sponsor.</p>\n\n<p>Let me know what I missed.</p>\n\n<p>Thanks for reading.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.junaideffendi.com/p/pinterest-data-tech-stack?r=cqjft&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?auto=webp&s=2c73e8b91ea311a42d09d9123dd7ec2ba4d36ee6', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=108&crop=smart&auto=webp&s=8f6ad94aa7bec1db2b7885842dece052bfffb4dc', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=216&crop=smart&auto=webp&s=d66c23cf9aaa4ff9a51f3edd35c8762dca2eb342', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=320&crop=smart&auto=webp&s=94bfa4942e7ef3580eb0022862bf641cf37ed9de', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=640&crop=smart&auto=webp&s=8f7960509ecba20ae5c636a66d9b18de9ed95cb2', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=960&crop=smart&auto=webp&s=226be44b4756473a09af6653ba44c1bbbd9c98ce', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/fFhPRWsQklwttunO4zodvXpUTmkUnUA_3zpPRcrys3o.jpg?width=1080&crop=smart&auto=webp&s=dee36a1d089d83d1e5ccce5c1d17b2239e314809', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'kigUZ-axf7c_mrYy07uOdmg5R9d0Uvtg5ViYuWlkXmg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1i50mw6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mjfnd'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i50mw6/pinterest_data_tech_stack/', 'stickied': False, 'url': 'https://www.junaideffendi.com/p/pinterest-data-tech-stack?r=cqjft&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false', 'subreddit_subscribers': 246994, 'created_utc': 1737299676.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.074+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I want to utilise a supervised ML model to score users based on their transactional data (the data contains both raw transaction data and analysed data - done by the third party ). it would be a supervised model, I have decided to use 30+ dpd in the next 6 months from the last transaction date as the indicator for good/bad behaviour. Any resources or Suggestions would be helpful. posted this on learnmachinelearning as well, but hoping to get resources from here too. Thanks in advance', 'author_fullname': 't2_zbb9w28mj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need Help Finding resources for transactional data modeling', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4x07k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737288202.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to utilise a supervised ML model to score users based on their transactional data (the data contains both raw transaction data and analysed data - done by the third party ). it would be a supervised model, I have decided to use 30+ dpd in the next 6 months from the last transaction date as the indicator for good/bad behaviour. Any resources or Suggestions would be helpful. posted this on learnmachinelearning as well, but hoping to get resources from here too. Thanks in advance</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4x07k', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='shreekar-h'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4x07k/need_help_finding_resources_for_transactional/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4x07k/need_help_finding_resources_for_transactional/', 'subreddit_subscribers': 246994, 'created_utc': 1737288202.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.074+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'We are revamping our scheduler which are cron jobs with Airflow/Dagster.\nThe requirements are \n1. All piplines across projects should be visible in a single UI.\n2. The pipeline code will be a docker image that the orchestrator should pull and run as scheduled.\n3. The individual tasks within a pipeline, i.e python functions, dbt models etc. should also be visible in the UI.(lineage within the pipeline)\n4. The orchestrator will be in a seperate instance and should execute the docker images in other seperate instances.\n5. The orchestrator code should live in the orchestrator instance and must be separated from pipeline code.\n6. All logs should be stored in the orchestrator instance. \n\nWe currently use Python for EL and dbt for T.\n\nQuestions:\n1. Are these requirements appropriate and make sense?\n2.Are these possible with Airflow/Dagster? \n3.Which of the two orchestrators would be more suitable for the requirements?\n4.What is the best way to go about setting this up that fulfills all these requirements?\n5. Are there better ways to go about this that I am overlooking?', 'author_fullname': 't2_w16t5qhn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help with orchestration[Airflow/Dagster]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4nnb9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737252600.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We are revamping our scheduler which are cron jobs with Airflow/Dagster.\nThe requirements are \n1. All piplines across projects should be visible in a single UI.\n2. The pipeline code will be a docker image that the orchestrator should pull and run as scheduled.\n3. The individual tasks within a pipeline, i.e python functions, dbt models etc. should also be visible in the UI.(lineage within the pipeline)\n4. The orchestrator will be in a seperate instance and should execute the docker images in other seperate instances.\n5. The orchestrator code should live in the orchestrator instance and must be separated from pipeline code.\n6. All logs should be stored in the orchestrator instance. </p>\n\n<p>We currently use Python for EL and dbt for T.</p>\n\n<p>Questions:\n1. Are these requirements appropriate and make sense?\n2.Are these possible with Airflow/Dagster? \n3.Which of the two orchestrators would be more suitable for the requirements?\n4.What is the best way to go about setting this up that fulfills all these requirements?\n5. Are there better ways to go about this that I am overlooking?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4nnb9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='booberrypie_'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1i4nnb9/help_with_orchestrationairflowdagster/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4nnb9/help_with_orchestrationairflowdagster/', 'subreddit_subscribers': 246994, 'created_utc': 1737252600.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.075+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'After completing "Fundamentals of Data Engineering" by Reis & Housley, I\'m building my first real DE project and looking for guidance on tool selection and best practices.\n\n**Project Overview:**\n\n* Building pipelines to collect IoT sensor environmental data via API\n* Cleaning data and engineering features\n* Storing in TimeScaleDB (Postgres) with hypertables\n* Generating automated weekly reports comparing 2-year running averages with current week\'s data\n* Using Python and SQL (intermediate Python, learning SQL)\n\nThe book emphasizes building custom solutions only when they provide competitive advantage, otherwise leveraging existing tools. This leads to my questions:\n\n1. How do you identify suitable open-source tools for a project like this?\n2. Any tips for finding and adapting similar GitHub projects?\n3. What are some recommended books/blogs/resources for someone learning DE independently?\n4. How would this type of project typically be structured in a professional setting?\n\nI\'m self-teaching and using LLMs to help with coding, but I\'m more focused on understanding proper frameworks and approaches. Any guidance on professional best practices would be greatly appreciated!', 'author_fullname': 't2_m2in1lh6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'First DE Project as Self Taught Developer: Seeking Tools & Best Practices for IoT Data Pipeline', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1i4cv8z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737222147.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>After completing &quot;Fundamentals of Data Engineering&quot; by Reis &amp; Housley, I&#39;m building my first real DE project and looking for guidance on tool selection and best practices.</p>\n\n<p><strong>Project Overview:</strong></p>\n\n<ul>\n<li>Building pipelines to collect IoT sensor environmental data via API</li>\n<li>Cleaning data and engineering features</li>\n<li>Storing in TimeScaleDB (Postgres) with hypertables</li>\n<li>Generating automated weekly reports comparing 2-year running averages with current week&#39;s data</li>\n<li>Using Python and SQL (intermediate Python, learning SQL)</li>\n</ul>\n\n<p>The book emphasizes building custom solutions only when they provide competitive advantage, otherwise leveraging existing tools. This leads to my questions:</p>\n\n<ol>\n<li>How do you identify suitable open-source tools for a project like this?</li>\n<li>Any tips for finding and adapting similar GitHub projects?</li>\n<li>What are some recommended books/blogs/resources for someone learning DE independently?</li>\n<li>How would this type of project typically be structured in a professional setting?</li>\n</ol>\n\n<p>I&#39;m self-teaching and using LLMs to help with coding, but I&#39;m more focused on understanding proper frameworks and approaches. Any guidance on professional best practices would be greatly appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i4cv8z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Data_OnThe_HalfShell'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i4cv8z/first_de_project_as_self_taught_developer_seeking/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i4cv8z/first_de_project_as_self_taught_developer_seeking/', 'subreddit_subscribers': 246994, 'created_utc': 1737222147.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.075+0000] {logging_mixin.py:190} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff77f562c0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Any one know where I can find comprehensive resources for Designing an end to end data ingestion pipeline. I want to ensure all things like designing, choosing tools, and scaling. Most YouTube videos aren’t that good and say generic stuff.  Thanks in advance. ', 'author_fullname': 't2_5h71s9gx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data/System Design Resources', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1i50j2w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1737299391.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Any one know where I can find comprehensive resources for Designing an end to end data ingestion pipeline. I want to ensure all things like designing, choosing tools, and scaling. Most YouTube videos aren’t that good and say generic stuff.  Thanks in advance. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1i50j2w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Smart-Ad9387'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1i50j2w/datasystem_design_resources/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1i50j2w/datasystem_design_resources/', 'subreddit_subscribers': 246994, 'created_utc': 1737299391.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-01-19T17:02:57.076+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-01-19T17:02:57.091+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-01-19T17:02:57.092+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2025-01-19T17:02:54.091393+00:00, execution_date=20250119T170254, start_date=20250119T170255, end_date=20250119T170257
[2025-01-19T17:02:57.157+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-01-19T17:02:57.178+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-01-19T17:02:57.180+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
